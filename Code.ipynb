{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import fetch_openml\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n"
      ],
      "metadata": {
        "id": "E0mzDrIkRYWX"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "id": "mJlwAF8rRPFP"
      },
      "outputs": [],
      "source": [
        "def generate_weights(num_layers, input_size, num_nodes):\n",
        "    weight_matrices = []\n",
        "\n",
        "    # Creating the initial_layer\n",
        "    initial_layer = [[0] * input_size for _ in range(num_nodes)]\n",
        "    weight_matrices.append(initial_layer)\n",
        "\n",
        "    # Creating the intermediate layers\n",
        "    intermediate_layer = [[0] * num_nodes for _ in range(num_nodes)]\n",
        "    for _ in range(num_layers - 2):\n",
        "        weight_matrices.append(intermediate_layer)\n",
        "\n",
        "    return weight_matrices\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_NN(num_neurons_hidden, num_neurons_output, num_input_samples):\n",
        "    input_nodes = [[0] * 3 for _ in range(num_input_samples)]\n",
        "    hidden_nodes = [[0] * 3 for _ in range(num_neurons_hidden)]\n",
        "    output_nodes = [[0] * 3 for _ in range(num_neurons_output)]\n",
        "    return [input_nodes, hidden_nodes, output_nodes]\n"
      ],
      "metadata": {
        "id": "Q5rdwp-DRg3T"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid_activation(x):\n",
        "    return 1 / (1 + np.exp(-x))\n"
      ],
      "metadata": {
        "id": "x-0gq93NTcqq"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propagation(weights, nodes):\n",
        "    for current_layer_index in range(len(nodes) - 1):\n",
        "        for current_node_index in range(len(nodes[current_layer_index + 1])):\n",
        "            total_weighted_sum = 0\n",
        "            for i in range(len(nodes[current_layer_index])):\n",
        "                total_weighted_sum += nodes[current_layer_index][i][1] * weights[current_layer_index][current_node_index][i]\n",
        "            nodes[current_layer_index + 1][current_node_index][0] = total_weighted_sum\n",
        "            nodes[current_layer_index + 1][current_node_index][1] = sigmoid_activation(nodes[current_layer_index + 1][current_node_index][0])\n",
        "    return nodes\n"
      ],
      "metadata": {
        "id": "1Kus8g2nTvRi"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backpropagation(input_nodes, weights, target, learning_rate):\n",
        "    for current_layer in range(len(input_nodes) - 1, -1, -1):\n",
        "        for current_node in range(len(input_nodes[current_layer])):\n",
        "            if current_layer == len(input_nodes) - 1:\n",
        "                sigma = input_nodes[current_layer][current_node][1] * (1 - input_nodes[current_layer][current_node][1]) * (target[current_node] - input_nodes[current_layer][current_node][1])\n",
        "                input_nodes[current_layer][current_node][2] = sigma\n",
        "                if current_layer != 0:\n",
        "                    for prev_node in range(len(input_nodes[current_layer - 1])):\n",
        "                        delta_weight = learning_rate * input_nodes[current_layer - 1][prev_node][1] * sigma\n",
        "                        weights[current_layer - 1][current_node][prev_node] += delta_weight\n",
        "            else:\n",
        "                weighted_sum = sum(input_nodes[current_layer + 1][k][2] * weights[current_layer][k][current_node] for k in range(len(input_nodes[current_layer + 1])))\n",
        "                sigma = input_nodes[current_layer][current_node][1] * (1 - input_nodes[current_layer][current_node][1]) * weighted_sum\n",
        "                input_nodes[current_layer][current_node][2] = sigma\n",
        "                if current_layer != 0:\n",
        "                    for prev_node in range(len(input_nodes[current_layer - 1])):\n",
        "                        delta_weight = learning_rate * input_nodes[current_layer - 1][prev_node][1] * sigma\n",
        "                        weights[current_layer - 1][current_node][prev_node] += delta_weight\n",
        "\n",
        "    return weights\n"
      ],
      "metadata": {
        "id": "akCxv6UHUzie"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris = datasets.load_iris()\n",
        "iris_X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "iris_Y = pd.get_dummies(iris.target)\n",
        "iris_X_list=iris_X.values.tolist()\n",
        "iris_Y_list=iris_Y.values.tolist()\n"
      ],
      "metadata": {
        "id": "fjj2n1yoWZzY"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mnist = fetch_openml('mnist_784')\n",
        "\n",
        "# # Convert to DataFrame and normalize the pixel values\n",
        "# mnist_X = pd.DataFrame(mnist.data / 255.0)\n",
        "# mnist_Y = pd.get_dummies(mnist.target)\n",
        "# mnist_data = pd.concat([mnist_X, mnist_Y], axis=1)\n",
        "\n",
        "# # Shuffle the concatenated data\n",
        "# mnist_data_shuffled = shuffle(mnist_data)\n",
        "\n",
        "# # Take the first 1000 samples after shuffling\n",
        "# mnist_data_subset = mnist_data_shuffled.head(1000)\n",
        "\n",
        "# # Separate features and target variable after shuffling\n",
        "# mnist_X_subset = mnist_data_subset.iloc[:, :784]  # Assuming 784 is the number of features\n",
        "# mnist_Y_subset = mnist_data_subset.iloc[:, 784:]\n",
        "# mnist_X_list = mnist_X_subset.values.tolist()\n",
        "# mnist_Y_list = mnist_Y_subset.values.tolist()\n",
        "#len(mnist_X_list)"
      ],
      "metadata": {
        "id": "IJ22mn8wW-2m"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2W1cs4X8DOhd"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_data = iris_X_list\n",
        "target_data = iris_Y_list\n",
        "\n",
        "learning_rate = 3\n",
        "num_layers = 3\n",
        "y_prid_list = []\n",
        "weights = generate_weights(num_layers, len(input_data[0]), len(target_data[0]))\n",
        "#print(weights)\n",
        "for input_sample, target in zip(input_data, target_data):\n",
        "\n",
        "    nodes = generate_NN(len(target), len(target), len(input_sample))\n",
        "\n",
        "    for k,inp in zip( nodes[0],input_sample):\n",
        "        k[1] = inp\n",
        "\n",
        "    for i in range(20):\n",
        "        nodes = forward_propagation(weights, nodes)\n",
        "        weights = backpropagation(nodes, weights, target, learning_rate)\n",
        "\n",
        "    y_list = [nodes[2][node][1] for node in range(len(nodes[2]))]\n",
        "    y_prid_list.append(y_list)\n",
        "\n",
        "columns = [f\"col{i}\" for i in range(1, len(y_prid_list[0]) + 1)]\n",
        "rounded_pred_list = [[int(round(value)) for value in row] for row in y_prid_list]\n",
        "predicted_output = pd.DataFrame(rounded_pred_list, columns=columns)\n",
        "\n",
        "#accuracy = accuracy_score(mnist_Y_subset[0:1000], predicted_output)\n",
        "accuracy = accuracy_score(iris_Y, predicted_output)\n",
        "print(\"Accuracy Score:\", accuracy * 100, \"%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yoJ2OyvRXgL",
        "outputId": "69532bd3-8642-4813-e3c2-4e43449b5b10"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score: 99.33333333333333 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "kAM1gxVOJnzQ"
      },
      "execution_count": 181,
      "outputs": []
    }
  ]
}